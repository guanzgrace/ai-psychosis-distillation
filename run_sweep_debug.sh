#!/bin/bash

# --- CONFIGURATION ---
MODELS=(
    "openrouter/google/gemma-3-27b-it"
    # "openrouter/deepseek/deepseek-v3.2"
    # "openrouter/meta-llama/llama-4-maverick"
    # "openrouter/anthropic/claude-haiku-4.5"
)

TASK_ENTRY="tasks/affective_eval.py@affective_sycophancy_eval"
BASE_LOG_DIR="./logs/debug3"
DATASET_PATH="../data/task2/processed/distilled_questions.json"
SAMPLE_LIMIT=1 # Limit the dataset to the first X examples, also needed as a safety harness for the Inspect CLI


for MODEL in "${MODELS[@]}"
do
    echo "------------------------------------------------"
    echo "DEBUG eval for $MODEL..."
    
    # 1. Standard Eval (No CoT, No Critique)
    echo "Mode: Standard"
    PYTHONPATH=. inspect eval "$TASK_ENTRY" \
        --model "$MODEL" \
        -T n=$SAMPLE_LIMIT \
        -T json_dataset_path="$DATASET_PATH" \
        --limit $SAMPLE_LIMIT \
        --log-dir "$BASE_LOG_DIR/standard"
        
    # 2. CoT Eval (Thinking enabled)
    echo "Mode: Chain of Thought"
    PYTHONPATH=. inspect eval "$TASK_ENTRY" \
        --model "$MODEL" \
        -T n=$SAMPLE_LIMIT \
        -T json_dataset_path="$DATASET_PATH" \
        -T use_cot=True \
        --limit $SAMPLE_LIMIT \
        --log-dir "$BASE_LOG_DIR/cot"
        
    # 3. CoT + Self-Critique
    echo "Mode: Self-Critique"
    PYTHONPATH=. inspect eval "$TASK_ENTRY" \
        --model "$MODEL" \
        -T n=$SAMPLE_LIMIT \
        -T json_dataset_path="$DATASET_PATH" \
        -T use_cot=True \
        -T use_self_critique=True \
        -T self_critique_model="openrouter/openai/gpt-4o-mini" \
        --limit $SAMPLE_LIMIT \
        --log-dir "$BASE_LOG_DIR/critique"
done